{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization - converting human readable text into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not unicode?\n",
    "- one glyph as one token causes shorter actual context (less words generated while keeping the same context size)\n",
    "- attention will become inefficient and expensive\n",
    "- purely for speed and memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    " text = \"Lorem üåÄ ‰Ω†Â•Ω üéâ, –º–∏—Ä! „Åì„Çì„Å´„Å°„ÅØ üåè\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Lorem üåÄ ‰Ω†Â•Ω üéâ, –º–∏—Ä! „Åì„Çì„Å´„Å°„ÅØ üåè\n",
      "Len: 26\n",
      "---\n",
      "Tokens: [76, 111, 114, 101, 109, 32, 240, 159, 140, 128, 32, 228, 189, 160, 229, 165, 189, 32, 240, 159, 142, 137, 44, 32, 208, 188, 208, 184, 209, 128, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 32, 240, 159, 140, 143]\n",
      "Len: 52\n"
     ]
    }
   ],
   "source": [
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(tokens) # list of integers\n",
    "\n",
    "print(f\"Text {text}\")\n",
    "print(f\"Len: {len(text)}\")\n",
    "print(\"---\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Len: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(tokens):\n",
    "    '''Get the number of times each pair of tokens appears in the text'''\n",
    "    stats = {}\n",
    "    for i in range(len(tokens)-1):\n",
    "        pair = (tokens[i], tokens[i+1])\n",
    "        stats[pair] = stats.get(pair, 0) + 1\n",
    "    stats = {k: v for k, v in sorted(stats.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurrences of each pair of tokens: {(227, 129): 4, (32, 240): 3, (240, 159): 3, (159, 140): 2, (147, 227): 2, (76, 111): 1, (111, 114): 1, (114, 101): 1, (101, 109): 1, (109, 32): 1, (140, 128): 1, (128, 32): 1, (32, 228): 1, (228, 189): 1, (189, 160): 1, (160, 229): 1, (229, 165): 1, (165, 189): 1, (189, 32): 1, (159, 142): 1, (142, 137): 1, (137, 44): 1, (44, 32): 1, (32, 208): 1, (208, 188): 1, (188, 208): 1, (208, 184): 1, (184, 209): 1, (209, 128): 1, (128, 33): 1, (33, 32): 1, (32, 227): 1, (129, 147): 1, (227, 130): 1, (130, 147): 1, (129, 171): 1, (171, 227): 1, (129, 161): 1, (161, 227): 1, (129, 175): 1, (175, 32): 1, (140, 143): 1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of occurrences of each pair of tokens: {get_stats(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens, pair_to_replace, new_token):\n",
    "    '''Replace all occurrences of a pair of tokens with a new token'''\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens)-1:\n",
    "        pair = (tokens[i], tokens[i+1])\n",
    "        if pair == pair_to_replace:\n",
    "            merged_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "            \n",
    "    if i == len(tokens)-1:\n",
    "        merged_tokens.append(tokens[-1])\n",
    "    \n",
    "    return merged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair to replace: (227, 129)\n",
      "New token: 999\n",
      "---\n",
      "Tokens: 52,  [76, 111, 114, 101, 109, 32, 240, 159, 140, 128, 32, 228, 189, 160, 229, 165, 189, 32, 240, 159, 142, 137, 44, 32, 208, 188, 208, 184, 209, 128, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 32, 240, 159, 140, 143]\n",
      "Merged: 48, [76, 111, 114, 101, 109, 32, 240, 159, 140, 128, 32, 228, 189, 160, 229, 165, 189, 32, 240, 159, 142, 137, 44, 32, 208, 188, 208, 184, 209, 128, 33, 32, 999, 147, 227, 130, 147, 999, 171, 999, 161, 999, 175, 32, 240, 159, 140, 143]\n"
     ]
    }
   ],
   "source": [
    "pair_to_replace = next(iter(get_stats(tokens))) # get the first element\n",
    "new_token = 999\n",
    "merged_tokens = merge(tokens, pair_to_replace, new_token)\n",
    "\n",
    "print(f\"Pair to replace: {pair_to_replace}\")\n",
    "print(f\"New token: {new_token}\")\n",
    "print(\"---\")\n",
    "print(f\"Tokens: {len(tokens)},  {tokens}\")\n",
    "print(f\"Merged: {len(merged_tokens)}, {merged_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
